{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks: reduce the dataset’s dimensionality (PCA, t-SNE, LLE, MDS, LDA), then apply classification(Logistic, SVM, Random Forest)"
      ],
      "metadata": {
        "id": "p44L2cAurIqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = fetch_openml('mnist_784', return_X_y=True, parser='auto')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "lxjsBVQj2ofe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "bK_tQv9f2vP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "from sklearn.manifold import TSNE, MDS, LocallyLinearEmbedding\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ============================================================================\n",
        "# 1. LOAD AND PREPARE DATA\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 1: LOADING MNIST DATASET\")\n",
        "print(\"=\"*70)\n",
        "start_total = time.time()\n",
        "\n",
        "X, y = fetch_openml('mnist_784', return_X_y=True, parser='auto')\n",
        "X = X / 255.0  # Normalize to [0, 1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"✓ Train set: {X_train.shape}\")\n",
        "print(f\"✓ Test set: {X_test.shape}\")\n",
        "print(f\"✓ Loading time: {time.time() - start_total:.2f}s\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. DIMENSIONALITY REDUCTION\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 2: DIMENSIONALITY REDUCTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "reduction_results = {}\n",
        "reduction_times = {}\n",
        "\n",
        "# --- 2.1 PCA ---\n",
        "print(\"\\n[1/6] PCA (50 components)...\")\n",
        "start = time.time()\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "elapsed = time.time() - start\n",
        "reduction_times['PCA'] = elapsed\n",
        "reduction_results['PCA'] = (X_train_pca, X_test_pca)\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s | Variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
        "\n",
        "# --- 2.2 Incremental PCA ---\n",
        "print(\"\\n[2/6] Incremental PCA (50 components)...\")\n",
        "start = time.time()\n",
        "ipca = IncrementalPCA(n_components=50, batch_size=200)\n",
        "X_train_ipca = ipca.fit_transform(X_train)\n",
        "X_test_ipca = ipca.transform(X_test)\n",
        "elapsed = time.time() - start\n",
        "reduction_times['IncrementalPCA'] = elapsed\n",
        "reduction_results['IncrementalPCA'] = (X_train_ipca, X_test_ipca)\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s\")\n",
        "\n",
        "# --- 2.3 LDA ---\n",
        "print(\"\\n[3/6] LDA (9 components - max for 10 classes)...\")\n",
        "start = time.time()\n",
        "lda = LinearDiscriminantAnalysis(n_components=9)\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "X_test_lda = lda.transform(X_test)\n",
        "elapsed = time.time() - start\n",
        "reduction_times['LDA'] = elapsed\n",
        "reduction_results['LDA'] = (X_train_lda, X_test_lda)\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s\")\n",
        "\n",
        "# --- 2.4 LLE (sampled) ---\n",
        "print(\"\\n[4/6] LLE (30 components, 10k samples - no transform available)...\")\n",
        "start = time.time()\n",
        "lle = LocallyLinearEmbedding(n_components=30, n_neighbors=10, random_state=42, n_jobs=-1)\n",
        "X_train_lle = lle.fit_transform(X_train[:10000])\n",
        "elapsed = time.time() - start\n",
        "reduction_times['LLE'] = elapsed\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s | Note: Cannot transform test set\")\n",
        "\n",
        "# --- 2.5 t-SNE (sampled) ---\n",
        "print(\"\\n[5/6] t-SNE (2 components, 5k samples - visualization only)...\")\n",
        "start = time.time()\n",
        "tsne = TSNE(n_components=2, random_state=42, n_jobs=-1, verbose=0)\n",
        "X_train_tsne = tsne.fit_transform(X_train[:5000])\n",
        "elapsed = time.time() - start\n",
        "reduction_times['t-SNE'] = elapsed\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s | Note: Cannot transform test set\")\n",
        "\n",
        "# --- 2.6 MDS (sampled) ---\n",
        "print(\"\\n[6/6] MDS (10 components, 2k samples - very slow)...\")\n",
        "start = time.time()\n",
        "mds = MDS(n_components=10, random_state=42, n_jobs=-1, verbose=0)\n",
        "X_train_mds = mds.fit_transform(X_train[:2000])\n",
        "elapsed = time.time() - start\n",
        "reduction_times['MDS'] = elapsed\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s | Note: Cannot transform test set\")\n",
        "\n",
        "# --- Summary Table ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DIMENSIONALITY REDUCTION TIMING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "timing_df = pd.DataFrame({\n",
        "    'Method': list(reduction_times.keys()),\n",
        "    'Time (s)': [f\"{t:.2f}\" for t in reduction_times.values()],\n",
        "    'Time (min)': [f\"{t/60:.2f}\" for t in reduction_times.values()],\n",
        "    'Can Transform Test': ['✓', '✓', '✓', '✗', '✗', '✗']\n",
        "})\n",
        "print(timing_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CLASSIFICATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: CLASSIFICATION (using PCA-reduced data)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "classification_results = {}\n",
        "\n",
        "# --- 3.1 Logistic Regression ---\n",
        "print(\"\\n[1/3] Logistic Regression...\")\n",
        "start = time.time()\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "lr.fit(X_train_pca, y_train)\n",
        "y_pred_lr = lr.predict(X_test_pca)\n",
        "elapsed = time.time() - start\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "classification_results['Logistic Regression'] = {'time': elapsed, 'accuracy': acc_lr}\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s | Accuracy: {acc_lr:.1%}\")\n",
        "\n",
        "# --- 3.2 SVM (subset for speed) ---\n",
        "print(\"\\n[2/3] SVM (5k samples for speed)...\")\n",
        "start = time.time()\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "svm.fit(X_train_pca[:5000], y_train[:5000])\n",
        "y_pred_svm = svm.predict(X_test_pca)\n",
        "elapsed = time.time() - start\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "classification_results['SVM'] = {'time': elapsed, 'accuracy': acc_svm}\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s | Accuracy: {acc_svm:.1%}\")\n",
        "\n",
        "# --- 3.3 Random Forest ---\n",
        "print(\"\\n[3/3] Random Forest...\")\n",
        "start = time.time()\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train_pca, y_train)\n",
        "y_pred_rf = rf.predict(X_test_pca)\n",
        "elapsed = time.time() - start\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "classification_results['Random Forest'] = {'time': elapsed, 'accuracy': acc_rf}\n",
        "print(f\"      ✓ Time: {elapsed:.2f}s | Accuracy: {acc_rf:.1%}\")\n",
        "\n",
        "# --- Classification Summary ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLASSIFICATION RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "clf_df = pd.DataFrame({\n",
        "    'Classifier': list(classification_results.keys()),\n",
        "    'Time (s)': [f\"{v['time']:.2f}\" for v in classification_results.values()],\n",
        "    'Accuracy': [f\"{v['accuracy']:.1%}\" for v in classification_results.values()]\n",
        "})\n",
        "print(clf_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 4. COMPARISON ACROSS REDUCTION METHODS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: LOGISTIC REGRESSION ON ALL REDUCTION METHODS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_results = {}\n",
        "for method, (X_tr, X_te) in reduction_results.items():\n",
        "    print(f\"\\nTesting {method}...\")\n",
        "    lr_temp = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "    lr_temp.fit(X_tr, y_train)\n",
        "    y_pred_temp = lr_temp.predict(X_te)\n",
        "    acc_temp = accuracy_score(y_test, y_pred_temp)\n",
        "    comparison_results[method] = acc_temp\n",
        "    print(f\"      ✓ Accuracy: {acc_temp:.1%}\")\n",
        "\n",
        "# --- Comparison Table ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ACCURACY COMPARISON TABLE\")\n",
        "print(\"=\"*70)\n",
        "comp_df = pd.DataFrame({\n",
        "    'Reduction Method': list(comparison_results.keys()),\n",
        "    'Components': [50, 50, 9],\n",
        "    'Accuracy': [f\"{v:.1%}\" for v in comparison_results.values()],\n",
        "    'Reduction Time': [f\"{reduction_times[k]:.2f}s\" for k in comparison_results.keys()]\n",
        "})\n",
        "print(comp_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "total_time = time.time() - start_total\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total execution time: {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
        "print(f\"\\nBest reduction method: {max(comparison_results, key=comparison_results.get)} \"\n",
        "      f\"({max(comparison_results.values()):.1%})\")\n",
        "print(f\"Best classifier: {max(classification_results, key=lambda k: classification_results[k]['accuracy'])} \"\n",
        "      f\"({max(v['accuracy'] for v in classification_results.values()):.1%})\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6AnhbjY2xyT",
        "outputId": "c67529bb-b38a-4963-a189-f53d69d21d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STEP 1: LOADING MNIST DATASET\n",
            "======================================================================\n",
            "✓ Train set: (56000, 784)\n",
            "✓ Test set: (14000, 784)\n",
            "✓ Loading time: 24.14s\n",
            "\n",
            "======================================================================\n",
            "STEP 2: DIMENSIONALITY REDUCTION\n",
            "======================================================================\n",
            "\n",
            "[1/6] PCA (50 components)...\n",
            "      ✓ Time: 1.90s | Variance: 82.6%\n",
            "\n",
            "[2/6] Incremental PCA (50 components)...\n",
            "      ✓ Time: 18.17s\n",
            "\n",
            "[3/6] LDA (9 components - max for 10 classes)...\n",
            "      ✓ Time: 19.00s\n",
            "\n",
            "[4/6] LLE (30 components, 10k samples - no transform available)...\n",
            "      ✓ Time: 182.74s | Note: Cannot transform test set\n",
            "\n",
            "[5/6] t-SNE (2 components, 5k samples - visualization only)...\n",
            "      ✓ Time: 56.66s | Note: Cannot transform test set\n",
            "\n",
            "[6/6] MDS (10 components, 2k samples - very slow)...\n",
            "      ✓ Time: 105.12s | Note: Cannot transform test set\n",
            "\n",
            "======================================================================\n",
            "DIMENSIONALITY REDUCTION TIMING SUMMARY\n",
            "======================================================================\n",
            "        Method Time (s) Time (min) Can Transform Test\n",
            "           PCA     1.90       0.03                  ✓\n",
            "IncrementalPCA    18.17       0.30                  ✓\n",
            "           LDA    19.00       0.32                  ✓\n",
            "           LLE   182.74       3.05                  ✗\n",
            "         t-SNE    56.66       0.94                  ✗\n",
            "           MDS   105.12       1.75                  ✗\n",
            "\n",
            "======================================================================\n",
            "STEP 3: CLASSIFICATION (using PCA-reduced data)\n",
            "======================================================================\n",
            "\n",
            "[1/3] Logistic Regression...\n",
            "      ✓ Time: 3.54s | Accuracy: 90.8%\n",
            "\n",
            "[2/3] SVM (5k samples for speed)...\n",
            "      ✓ Time: 3.17s | Accuracy: 96.0%\n",
            "\n",
            "[3/3] Random Forest...\n",
            "      ✓ Time: 75.87s | Accuracy: 95.3%\n",
            "\n",
            "======================================================================\n",
            "CLASSIFICATION RESULTS SUMMARY\n",
            "======================================================================\n",
            "         Classifier Time (s) Accuracy\n",
            "Logistic Regression     3.54    90.8%\n",
            "                SVM     3.17    96.0%\n",
            "      Random Forest    75.87    95.3%\n",
            "\n",
            "======================================================================\n",
            "STEP 4: LOGISTIC REGRESSION ON ALL REDUCTION METHODS\n",
            "======================================================================\n",
            "\n",
            "Testing PCA...\n",
            "      ✓ Accuracy: 90.8%\n",
            "\n",
            "Testing IncrementalPCA...\n",
            "      ✓ Accuracy: 90.8%\n",
            "\n",
            "Testing LDA...\n",
            "      ✓ Accuracy: 88.4%\n",
            "\n",
            "======================================================================\n",
            "ACCURACY COMPARISON TABLE\n",
            "======================================================================\n",
            "Reduction Method  Components Accuracy Reduction Time\n",
            "             PCA          50    90.8%          1.90s\n",
            "  IncrementalPCA          50    90.8%         18.17s\n",
            "             LDA           9    88.4%         19.00s\n",
            "\n",
            "======================================================================\n",
            "✓ ANALYSIS COMPLETE!\n",
            "======================================================================\n",
            "Total execution time: 498.70s (8.31 min)\n",
            "\n",
            "Best reduction method: PCA (90.8%)\n",
            "Best classifier: SVM (96.0%)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SUMMARY**\n",
        "This analysis involved a machine learning pipeline on the MNIST dataset, focusing on dimensionality reduction followed by classification.\n",
        "\n",
        "Step 1: Data Loading and Preparation. The MNIST_784 dataset was loaded, normalized, and split into training (56,000 samples) and testing (14,000 samples) sets.\n",
        "\n",
        "Step 2: Dimensionality Reduction. Several techniques were applied: PCA, Incremental PCA, LDA, LLE, t-SNE, and MDS. PCA (50 components) was the fastest (1.90s), explaining 82.6% of the variance, and, along with Incremental PCA and LDA, allowed for test set transformation.\n",
        "\n",
        "Step 3: Classification on PCA-reduced data. Three classifiers were trained: Logistic Regression, SVM, and Random Forest. SVM, trained on a subset of 5,000 samples, achieved the highest accuracy at 96.0% in 3.17s. Logistic Regression achieved 90.8%, and Random Forest achieved 95.3%.\n",
        "\n",
        "Step 4: Comparison across Reduction Methods. Logistic Regression was re-applied to data reduced by PCA, IncrementalPCA, and LDA. PCA and IncrementalPCA both yielded 90.8% accuracy, while LDA resulted in 88.4% accuracy.\n",
        "\n",
        "#Final Result and Interpretation:\n",
        "\n",
        "The analysis successfully demonstrated the impact of different dimensionality reduction techniques on classification performance. PCA emerged as the best reduction method for this task, offering a good balance of speed and classification accuracy (90.8% with Logistic Regression). Among the classifiers tested, SVM achieved the highest accuracy (96.0%) on the PCA-reduced data, demonstrating its effectiveness even with a sampled training set. This experiment highlights that simpler, linear reduction methods like PCA can be very efficient and effective for classification when combined with powerful classifiers like SVM, even on large datasets like MNIST."
      ],
      "metadata": {
        "id": "yS73I2J6xKpd"
      }
    }
  ]
}